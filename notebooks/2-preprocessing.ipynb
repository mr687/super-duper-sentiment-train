{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import swifter\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = 'tweets.csv'\n",
    "raw_data_path = f'../dataset/raw/raw_{target_dataset}'\n",
    "normalization_list_path = '../dataset/wordlist/normalization_list.csv'\n",
    "output_path = f'../dataset/processed/processed_{target_dataset}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7502, 27)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(raw_data_path, delimiter='`')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7502, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@soeyoto1 @msaid_didu Bukti kalau Pemerintahan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@KompasTV Mentri esdm, trus patokanmu apa? Kan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@hnurwahid Ngaku masih subsidi tapi ada swasta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Tan_Mar3M Kalau Vivo bisaberani bertahan deng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Menteri BUMN Erick Thohir menegaskan persiapan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  @soeyoto1 @msaid_didu Bukti kalau Pemerintahan...\n",
       "1  @KompasTV Mentri esdm, trus patokanmu apa? Kan...\n",
       "2  @hnurwahid Ngaku masih subsidi tapi ada swasta...\n",
       "3  @Tan_Mar3M Kalau Vivo bisaberani bertahan deng...\n",
       "4  Menteri BUMN Erick Thohir menegaskan persiapan..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['content']]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization list: 1034\n",
      "Indonesian stopwords: 758\n",
      "English stopwords: 179\n",
      "Custom stopwords: 110\n",
      "Total stopwords: 1047\n"
     ]
    }
   ],
   "source": [
    "# Functions to clean data\n",
    "\n",
    "def case_folding(text):\n",
    "\treturn text.lower()\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "\t# Remove @mentions\n",
    "\ttweet = re.sub(r'@\\S+', ' ', tweet)\n",
    "\t# Remove URLs\n",
    "\ttweet = re.sub(r'https?://[A-Za-z0-9./]+', ' ', tweet)\n",
    "\t# Remove RT\n",
    "\ttweet = re.sub(r'RT : ', ' ', tweet)\n",
    "\t# Remove punctuation\n",
    "\ttweet = re.sub(r'[^\\w\\s]', ' ', tweet)\n",
    "\t# Remove numbers\n",
    "\ttweet = re.sub(r'[0-9]', ' ', tweet)\n",
    "\t# Remove whitespace\n",
    "\ttweet = re.sub(r'\\s+', ' ', tweet)\n",
    "\t# Remove leading and trailing whitespace\n",
    "\ttweet = tweet.strip()\n",
    "\t# Remove non-ASCII characters\n",
    "\ttweet = tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "\t# Keep tweet with more than 2 characters\n",
    "\ttweet = ' '.join([w for w in tweet.split() if len(w) > 2])\n",
    "\treturn tweet\n",
    "\n",
    "def tokenize(tweet):\n",
    "\treturn nltk.word_tokenize(tweet)\n",
    "\n",
    "normalization_list = pd.read_csv(normalization_list_path, delimiter=',')\n",
    "list_normalize_targets = list(normalization_list['target'])\n",
    "list_normalize_replacements = list(normalization_list['replacement'])\n",
    "print(f\"Normalization list: {len(list_normalize_targets)}\")\n",
    "\n",
    "def normalize(tokens):\n",
    "\treturn [list_normalize_replacements[list_normalize_targets.index(token)] if token in list_normalize_targets else token for token in tokens]\n",
    "\n",
    "indonesian_stopwords_list = nltk.corpus.stopwords.words('indonesian')\n",
    "english_stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "custom_stopwords_list = ['mun','aya','ooo','nre','dur','tir','sih','aji','akun','akuuu','akuuuu','al','ala','alah',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'bruh','gokkkssss','kop','p','y','my','nih','in','r','hnw','lt','wkwkwk','sulawesi','alsut',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'alvien','andesten','anonymous','anonim','nya','blt','barcodenya','tyyy','this','lyfe','wkwkw',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'petronas','mas','bang','kak','eeeaaaa','sep','cokkk','wkwkwwk','and','indramayu','hayoo',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'hihi','rdp','neg','cc','ber','ybs','juta','ribu','dir','triliun','any','anything','anyway'\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'kom','miliar','tralalatrililiun','piis','thd','trilyunan','aok','aos','aowkwk','apwkwowkwokw',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'uud','non','lho','lha','lalo','kakak','dik','adik','adk','kitaji','halo','hai','²','½kali','aaaahhh',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'aaahh','aaahhh','aah','aatu','abad','abah','abdur','abrik','abrol','abt','adick','adi','adik','adol','aff','aga',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'aha','ahahah','ahahahaa','ahshshskaks','akn','akp','akr']\n",
    "\n",
    "list_stopwords = indonesian_stopwords_list + english_stopwords_list + custom_stopwords_list\n",
    "print(f\"Indonesian stopwords: {len(indonesian_stopwords_list)}\")\n",
    "print(f\"English stopwords: {len(english_stopwords_list)}\")\n",
    "print(f\"Custom stopwords: {len(custom_stopwords_list)}\")\n",
    "print(f\"Total stopwords: {len(list_stopwords)}\")\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "\treturn [token for token in tokens if token not in list_stopwords]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Case folding, data cleaning and tokenization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fc1cd462ef4b5f800cfe044c29730d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/7502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087b6a6d366b44f3a606c2db8e8eaa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/7502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64a4434c90d45dabb79b3762ecc893a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/7502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7502, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@soeyoto1 @msaid_didu Bukti kalau Pemerintahan...</td>\n",
       "      <td>[bukti, kalau, pemerintahan, pak, tidak, pro, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@KompasTV Mentri esdm, trus patokanmu apa? Kan...</td>\n",
       "      <td>[mentri, esdm, trus, patokanmu, apa, kantong, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@hnurwahid Ngaku masih subsidi tapi ada swasta...</td>\n",
       "      <td>[ngaku, masih, subsidi, tapi, ada, swasta, jua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Tan_Mar3M Kalau Vivo bisaberani bertahan deng...</td>\n",
       "      <td>[kalau, vivo, bisaberani, bertahan, dengan, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Menteri BUMN Erick Thohir menegaskan persiapan...</td>\n",
       "      <td>[menteri, bumn, erick, thohir, menegaskan, per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  @soeyoto1 @msaid_didu Bukti kalau Pemerintahan...   \n",
       "1  @KompasTV Mentri esdm, trus patokanmu apa? Kan...   \n",
       "2  @hnurwahid Ngaku masih subsidi tapi ada swasta...   \n",
       "3  @Tan_Mar3M Kalau Vivo bisaberani bertahan deng...   \n",
       "4  Menteri BUMN Erick Thohir menegaskan persiapan...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [bukti, kalau, pemerintahan, pak, tidak, pro, ...  \n",
       "1  [mentri, esdm, trus, patokanmu, apa, kantong, ...  \n",
       "2  [ngaku, masih, subsidi, tapi, ada, swasta, jua...  \n",
       "3  [kalau, vivo, bisaberani, bertahan, dengan, ha...  \n",
       "4  [menteri, bumn, erick, thohir, menegaskan, per...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = df['content'].swifter.apply(case_folding)\n",
    "df['tokens'] = df['tokens'].swifter.apply(clean_tweet)\n",
    "df['tokens'] = df['tokens'].swifter.apply(tokenize)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Normalization and Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e56916262c045aaa350d97fe82105b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/7502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff2261c680f4f9c89d2e4437b674faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/7502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7502, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@soeyoto1 @msaid_didu Bukti kalau Pemerintahan...</td>\n",
       "      <td>[bukti, pemerintahan, pro, rakyat, pro, oligar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@KompasTV Mentri esdm, trus patokanmu apa? Kan...</td>\n",
       "      <td>[menteri, esdm, patokanmu, kantong, pertamina,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@hnurwahid Ngaku masih subsidi tapi ada swasta...</td>\n",
       "      <td>[mengaku, subsidi, swasta, jual, murah, subsid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Tan_Mar3M Kalau Vivo bisaberani bertahan deng...</td>\n",
       "      <td>[vivo, berani, bertahan, harga, nasib, pertamina]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Menteri BUMN Erick Thohir menegaskan persiapan...</td>\n",
       "      <td>[menteri, bumn, erick, thohir, persiapan, indo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  @soeyoto1 @msaid_didu Bukti kalau Pemerintahan...   \n",
       "1  @KompasTV Mentri esdm, trus patokanmu apa? Kan...   \n",
       "2  @hnurwahid Ngaku masih subsidi tapi ada swasta...   \n",
       "3  @Tan_Mar3M Kalau Vivo bisaberani bertahan deng...   \n",
       "4  Menteri BUMN Erick Thohir menegaskan persiapan...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [bukti, pemerintahan, pro, rakyat, pro, oligar...  \n",
       "1  [menteri, esdm, patokanmu, kantong, pertamina,...  \n",
       "2  [mengaku, subsidi, swasta, jual, murah, subsid...  \n",
       "3  [vivo, berani, bertahan, harga, nasib, pertamina]  \n",
       "4  [menteri, bumn, erick, thohir, persiapan, indo...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = df['tokens'].swifter.apply(normalize)\n",
    "df['tokens'] = df['tokens'].swifter.apply(remove_stopwords)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Remove Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (7502, 2)\n",
      "After: (7104, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {df.shape}\")\n",
    "\n",
    "# Remove duplicate data\n",
    "df.drop_duplicates(subset='tokens', keep=\"first\", inplace=True)\n",
    "\n",
    "print(f\"After: {df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique terms: 11991\n",
      "On processing... 0 terms have been stemmed\n",
      "On processing... 1000 terms have been stemmed\n",
      "On processing... 2000 terms have been stemmed\n",
      "On processing... 3000 terms have been stemmed\n",
      "On processing... 4000 terms have been stemmed\n",
      "On processing... 5000 terms have been stemmed\n",
      "On processing... 6000 terms have been stemmed\n",
      "On processing... 7000 terms have been stemmed\n",
      "On processing... 8000 terms have been stemmed\n",
      "On processing... 9000 terms have been stemmed\n",
      "On processing... 10000 terms have been stemmed\n",
      "On processing... 11000 terms have been stemmed\n"
     ]
    }
   ],
   "source": [
    "# Functions to stem data\n",
    "# It will take more times to process (~20 minutes)\n",
    "\n",
    "stem_factory = StemmerFactory()\n",
    "stemmer = stem_factory.create_stemmer()\n",
    "\n",
    "terms_dict = {}\n",
    "for tokens in df['tokens']:\n",
    "\tfor token in tokens:\n",
    "\t\tif token not in terms_dict:\n",
    "\t\t\tterms_dict[token] = ''\n",
    "\n",
    "print(f\"Unique terms: {len(terms_dict)}\")\n",
    "\n",
    "for i, term in enumerate(terms_dict):\n",
    "\tterms_dict[term] = stemmer.stem(f'{term}')\n",
    "\tif i % 1000 == 0:\n",
    "\t\tprint(f\"On processing... {i} terms have been stemmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2837ed0093b4545af0f91787a33e8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/7104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7104, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_ready</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@soeyoto1 @msaid_didu Bukti kalau Pemerintahan...</td>\n",
       "      <td>[bukti, pemerintahan, pro, rakyat, pro, oligar...</td>\n",
       "      <td>[bukti, perintah, pro, rakyat, pro, oligarki, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@KompasTV Mentri esdm, trus patokanmu apa? Kan...</td>\n",
       "      <td>[menteri, esdm, patokanmu, kantong, pertamina,...</td>\n",
       "      <td>[menteri, esdm, patok, kantong, pertamina, jeb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@hnurwahid Ngaku masih subsidi tapi ada swasta...</td>\n",
       "      <td>[mengaku, subsidi, swasta, jual, murah, subsid...</td>\n",
       "      <td>[aku, subsidi, swasta, jual, murah, subsidi, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Tan_Mar3M Kalau Vivo bisaberani bertahan deng...</td>\n",
       "      <td>[vivo, berani, bertahan, harga, nasib, pertamina]</td>\n",
       "      <td>[vivo, berani, tahan, harga, nasib, pertamina]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Menteri BUMN Erick Thohir menegaskan persiapan...</td>\n",
       "      <td>[menteri, bumn, erick, thohir, persiapan, indo...</td>\n",
       "      <td>[menteri, bumn, erick, thohir, siap, indonesia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  @soeyoto1 @msaid_didu Bukti kalau Pemerintahan...   \n",
       "1  @KompasTV Mentri esdm, trus patokanmu apa? Kan...   \n",
       "2  @hnurwahid Ngaku masih subsidi tapi ada swasta...   \n",
       "3  @Tan_Mar3M Kalau Vivo bisaberani bertahan deng...   \n",
       "4  Menteri BUMN Erick Thohir menegaskan persiapan...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [bukti, pemerintahan, pro, rakyat, pro, oligar...   \n",
       "1  [menteri, esdm, patokanmu, kantong, pertamina,...   \n",
       "2  [mengaku, subsidi, swasta, jual, murah, subsid...   \n",
       "3  [vivo, berani, bertahan, harga, nasib, pertamina]   \n",
       "4  [menteri, bumn, erick, thohir, persiapan, indo...   \n",
       "\n",
       "                                        tokens_ready  \n",
       "0  [bukti, perintah, pro, rakyat, pro, oligarki, ...  \n",
       "1  [menteri, esdm, patok, kantong, pertamina, jeb...  \n",
       "2  [aku, subsidi, swasta, jual, murah, subsidi, p...  \n",
       "3     [vivo, berani, tahan, harga, nasib, pertamina]  \n",
       "4  [menteri, bumn, erick, thohir, siap, indonesia...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_stemmed_tokens(tokens):\n",
    "\treturn [terms_dict[token] for token in tokens]\n",
    "\n",
    "df['tokens_ready'] = df['tokens'].swifter.apply(apply_stemmed_tokens)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_path, index=False, header=True, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 18:25:35) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85fe51655c1fcaa9a7b9c0223583abd4e8b4409b0fbdb1dc760dec4edd874462"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
